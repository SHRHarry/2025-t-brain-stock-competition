import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, classification_report
from lightgbm import LGBMClassifier
from imblearn.over_sampling import SMOTE
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

def train(train_path):
    # === 1. è¼‰å…¥è³‡æ–™ ===
    df = pd.read_csv(train_path)
    
    # === 2. ç§»é™¤ç¼ºå¤±ç‡ > 90% çš„æ¬„ä½ ===
    df = df.loc[:, df.isnull().mean() < 0.9]

    # === 3. ä¿ç•™éç©ºç‡å‰ 300 åç‰¹å¾µ + æ¨™ç±¤æ¬„ä½ ===
    non_null_ratio = df.notnull().mean().sort_values(ascending=False)
    top_columns = non_null_ratio.head(300).index.tolist()
    if "é£†è‚¡" not in top_columns:
        top_columns.append("é£†è‚¡")
    df = df[top_columns]

    # === 4. å»é™¤éæ•¸å€¼æ¬„ä½ + åˆ‡åˆ†è³‡æ–™ ===
    df = df.select_dtypes(include=["number"])
    X = df.drop(columns=["é£†è‚¡"])
    y = df["é£†è‚¡"]

    # è‹¥ y ä¸­åªæœ‰ä¸€é¡ï¼Œç›´æ¥è·³å‡ºæç¤º
    if y.nunique() < 2:
        raise ValueError("è³‡æ–™ä¸­åƒ…åŒ…å«å–®ä¸€é¡åˆ¥ï¼Œç„¡æ³•è¨“ç·´æ¨¡å‹")

    # === 5. å¡«è£œç¼ºå¤±å€¼ï¼ˆä¸­ä½æ•¸ï¼‰ ===
    imputer = SimpleImputer(strategy="median")
    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

    # === 6. åˆ‡åˆ†è³‡æ–™ ===
    # æ‰¾ä¸€çµ„åŒ…å«å…©é¡çš„åˆ‡æ³•ï¼ˆé¿å… SMOTE éŒ¯èª¤ï¼‰
    for seed in range(100):
        X_train, X_test, y_train, y_test = train_test_split(
            X_imputed, y, stratify=y, test_size=0.2, random_state=seed
        )
        if y_train.nunique() == 2:
            print(f"[âœ“] æˆåŠŸæ‰¾åˆ°åŒ…å«å…©é¡åˆ¥çš„è¨“ç·´é›† seed = {seed}")
            break
    else:
        raise ValueError("ç„¡æ³•æ‰¾åˆ°åŒæ™‚åŒ…å« 0/1 çš„è¨“ç·´è³‡æ–™")

    # === 7. SMOTE ä¸Šå–æ¨£ ===
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # === 8. LightGBM è¨“ç·´ï¼ˆè™•ç†ä¸å¹³è¡¡é¡åˆ¥ï¼‰ ===
    model = LGBMClassifier(random_state=42, is_unbalance=True)
    model.fit(X_train_resampled, y_train_resampled)

    # === 9. é æ¸¬èˆ‡è©•ä¼° ===
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    f1 = f1_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    print("\n=== ğŸ“Š è©•ä¼°çµæœ ===")
    print(f"Accuracy: {acc:.4f}")
    print(f"AUC: {auc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(report)

    # === 10. å„²å­˜æ¨¡å‹èˆ‡å‰è™•ç†å™¨ ===
    joblib.dump(model, "lgbm_model.pkl")
    joblib.dump(imputer, "imputer.pkl")
    print("\nâœ… æ¨¡å‹èˆ‡ Imputer å·²å„²å­˜ï¼")
    '''
        === ğŸ“Š è©•ä¼°çµæœ ===
    Accuracy: 0.9945
    AUC: 0.9818
    F1 Score: 0.5714
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     39879
           1       0.67      0.50      0.57       294

    accuracy                           0.99     40173
   macro avg       0.83      0.75      0.78     40173
weighted avg       0.99      0.99      0.99     40173
'''

def feature_selection_train(train_path, use_pretrained=False):
    # === 1. è®€å–è³‡æ–™ ===
    df = pd.read_csv(train_path)

    # === 2. æ¸…æ´—èˆ‡å‰è™•ç†ï¼ˆèˆ‡åŸå§‹æµç¨‹ä¸€è‡´ï¼‰ ===
    df = df.loc[:, df.isnull().mean() < 0.9]
    non_null_ratio = df.notnull().mean().sort_values(ascending=False)
    top_columns = non_null_ratio.head(300).index.tolist()
    if "é£†è‚¡" not in top_columns:
        top_columns.append("é£†è‚¡")
    df = df[top_columns]
    df = df.select_dtypes(include=["number"])

    X = df.drop(columns=["é£†è‚¡"])
    y = df["é£†è‚¡"]

    if use_pretrained:
        # === 3. å¡«è£œç¼ºå¤±å€¼ï¼ˆä¸­ä½æ•¸ï¼‰ ===
        model_original = joblib.load("lgbm_model.pkl")
        imputer_original = joblib.load("imputer.pkl")
        X_imputed = pd.DataFrame(imputer_original.transform(X), columns=X.columns)

        # === 4. ä½¿ç”¨åŸæ¨¡å‹çš„é‡è¦æ€§é¸å‡º Top N ç‰¹å¾µ ===
        importances = model_original.feature_importances_
    else:
        # === 3. å¡«è£œç¼ºå¤±å€¼ï¼ˆä¸­ä½æ•¸ï¼‰ ===
        imputer = SimpleImputer(strategy="median")
        X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

        # === 4. ä½¿ç”¨åŸå§‹æ¨¡å‹æ‰¾å‡º top N ç‰¹å¾µ ===
        base_model = LGBMClassifier(random_state=42, is_unbalance=True)
        base_model.fit(X_imputed, y)
        importances = base_model.feature_importances_
    
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': importances
    }).sort_values(by='importance', ascending=False)

    top_n = 50
    top_features = importance_df.head(top_n)["feature"].tolist()

    # === 5. ç•«åœ–é¡¯ç¤ºå‰ 50 ç‰¹å¾µ ===
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.figure(figsize=(10, 12))
    sns.barplot(data=importance_df.head(top_n), y="feature", x="importance", palette="viridis")
    
    if use_pretrained:
        plt.title(f"Top {top_n} Feature Importances (From Trained Model)")
        plt.tight_layout()
        plt.savefig("top_features_from_original_model.png")
    else:
        plt.title(f"Top {top_n} Feature Importances")
        plt.tight_layout()
        plt.savefig("top_features.png")
    print(f"âœ… å·²å„²å­˜å‰ {top_n} ç‰¹å¾µåœ– top_features.png")

    # === 6. ä½¿ç”¨ top N ç‰¹å¾µé‡å»ºè³‡æ–™èˆ‡ Imputer ===
    X_top = X[top_features]
    imputer_top = SimpleImputer(strategy="median")
    X_top_imputed = pd.DataFrame(imputer_top.fit_transform(X_top), columns=X_top.columns)

    # === 7. åˆ‡åˆ†è¨“ç·´èˆ‡æ¸¬è©¦é›† ===
    for seed in range(100):
        X_train, X_test, y_train, y_test = train_test_split(
            X_top_imputed, y, stratify=y, test_size=0.2, random_state=seed)
        if y_train.nunique() == 2:
            break

    # === 8. SMOTE è™•ç†ä¸å¹³è¡¡è³‡æ–™ ===
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # === 9. è¨“ç·´æ–°æ¨¡å‹ ===
    model_top = LGBMClassifier(random_state=42, is_unbalance=True)
    model_top.fit(X_train_resampled, y_train_resampled)

    # === 10. è©•ä¼°æ–°æ¨¡å‹ ===
    y_pred = model_top.predict(X_test)
    y_prob = model_top.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    f1 = f1_score(y_test, y_pred)

    if use_pretrained:
        print("\n=== ğŸ“Š ä½¿ç”¨åŸæ¨¡å‹ importance çš„ Top Features è©•ä¼°çµæœ ===")
        print(f"Accuracy: {acc:.4f}")
        print(f"AUC: {auc:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(classification_report(y_test, y_pred))

        # === 10. å„²å­˜æ¨¡å‹èˆ‡å‰è™•ç†å™¨ ===
        joblib.dump(model_top, "lgbm_model_top_from_original.pkl")
        joblib.dump(imputer_top, "imputer_top_from_original.pkl")
        joblib.dump(top_features, "top_features_from_original.pkl")
        print("âœ… æ¨¡å‹ã€Imputerã€Top Features å·²å„²å­˜ (from original model)")
        '''
        === ğŸ“Š ä½¿ç”¨åŸæ¨¡å‹ importance çš„ Top Features è©•ä¼°çµæœ ===
        Accuracy: 0.9948
        AUC: 0.9800
        F1 Score: 0.5625
                      precision    recall  f1-score   support

                   0       1.00      1.00      1.00     39879
                   1       0.73      0.46      0.56       294

            accuracy                           0.99     40173
           macro avg       0.86      0.73      0.78     40173
        weighted avg       0.99      0.99      0.99     40173
        '''
    else:
        print("\n=== ğŸ“Š Top Features æ¨¡å‹è©•ä¼°çµæœ ===")
        print(f"Accuracy: {acc:.4f}")
        print(f"AUC: {auc:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(classification_report(y_test, y_pred))

        # === 11. å„²å­˜æ¨¡å‹èˆ‡ Imputer èˆ‡ top feature æ¬„ä½å ===
        joblib.dump(model_top, "lgbm_model_top.pkl")
        joblib.dump(imputer_top, "imputer_top.pkl")
        joblib.dump(top_features, "top_features.pkl")
        print("âœ… æ¨¡å‹ã€Imputer èˆ‡æ¬„ä½åç¨±å·²å„²å­˜")
        '''
            === ğŸ“Š Top Features æ¨¡å‹è©•ä¼°çµæœ ===
        Accuracy: 0.9777
        AUC: 0.9836
        F1 Score: 0.3421
                      precision    recall  f1-score   support

                   0       1.00      0.98      0.99     39879
                   1       0.22      0.79      0.34       294

            accuracy                           0.98     40173
           macro avg       0.61      0.89      0.67     40173
        weighted avg       0.99      0.98      0.98     40173
        '''

if __name__ == "__main__":
    # train(r"E:\side_project\stock_competition\data\38_Training_Data_Set\training.csv")
    feature_selection_train(r"E:\side_project\stock_competition\data\38_Training_Data_Set\training.csv",
                            use_pretrained=True)